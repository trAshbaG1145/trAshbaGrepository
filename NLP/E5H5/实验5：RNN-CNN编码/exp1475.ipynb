{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "实验\n",
    "1.样例代码avrPooling.html，LSTM-classification.html分别采用平均池化和LSTM模型最后的隐层向量对句子编码并分类。请参照给出的代码，编写如下二分类模型：对LSTM的每一步输出向量进行平均池化，作为句子的编码向量，实现train.txt（按照8：2划分训练和测试集）中的数据的二分类，请输出模型在测试集上的评测性能(precision, recall, f1)。\n",
    "\n",
    "2. TextCNN-2classes-torch.html，LSTM-classification.html分别采用CNN和LSTM架构对文本编码并实现二分类。请参照给出的代码，编写代码将CNN和LSTM整合对train.txt（8：2划分训练和测试数据）中的文本编码并二分类。整合方式可以是堆叠，向量平均，向量拼接。请输出模型在测试集上的评测性能(precision, recall, f1)，并跟上面的方法进行性能比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载数据 (train.txt)...\n",
      "数据加载完毕。训练集: 5534 条, 测试集: 1384 条。\n",
      "使用设备: cpu\n",
      "\n",
      "--- 开始 实验 1 (LSTM + 输出均值池化) ---\n",
      "Epoch 01/10, Loss: 0.2904\n",
      "Epoch 02/10, Loss: 0.0788\n",
      "Epoch 03/10, Loss: 0.0473\n",
      "Epoch 04/10, Loss: 0.0273\n",
      "Epoch 05/10, Loss: 0.0234\n",
      "Epoch 06/10, Loss: 0.0160\n",
      "Epoch 07/10, Loss: 0.0064\n",
      "Epoch 08/10, Loss: 0.0117\n",
      "Epoch 09/10, Loss: 0.0072\n",
      "Epoch 10/10, Loss: 0.0078\n",
      "实验 1 训练完成。\n",
      "\n",
      "--- 实验 1 评测报告 (LSTM + 所有步长输出平均池化) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      0.98      0.97       595\n",
      "     Class 1       0.98      0.98      0.98       789\n",
      "\n",
      "    accuracy                           0.98      1384\n",
      "   macro avg       0.98      0.98      0.98      1384\n",
      "weighted avg       0.98      0.98      0.98      1384\n",
      "\n",
      "\n",
      "--- 开始 实验 2 (CNN + LSTM 拼接) ---\n",
      "Epoch 01/10, Loss: 0.1587\n",
      "Epoch 02/10, Loss: 0.0439\n",
      "Epoch 03/10, Loss: 0.0262\n",
      "Epoch 04/10, Loss: 0.0145\n",
      "Epoch 05/10, Loss: 0.0090\n",
      "Epoch 06/10, Loss: 0.0068\n",
      "Epoch 07/10, Loss: 0.0026\n",
      "Epoch 08/10, Loss: 0.0020\n",
      "Epoch 09/10, Loss: 0.0023\n",
      "Epoch 10/10, Loss: 0.0010\n",
      "实验 2 训练完成。\n",
      "\n",
      "--- 实验 2 评测报告 (CNN + LSTM 拼接) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.97      0.98       595\n",
      "     Class 1       0.98      0.99      0.98       789\n",
      "\n",
      "    accuracy                           0.98      1384\n",
      "   macro avg       0.98      0.98      0.98      1384\n",
      "weighted avg       0.98      0.98      0.98      1384\n",
      "\n",
      "\n",
      "--- 实验 1 与 实验 2 性能比较 ---\n",
      "\n",
      "实验 1 结果 (LSTM + 输出均值池化):**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      0.98      0.97       595\n",
      "     Class 1       0.98      0.98      0.98       789\n",
      "\n",
      "    accuracy                           0.98      1384\n",
      "   macro avg       0.98      0.98      0.98      1384\n",
      "weighted avg       0.98      0.98      0.98      1384\n",
      "\n",
      "\n",
      "实验 2 结果 (CNN + LSTM 拼接):**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.97      0.98       595\n",
      "     Class 1       0.98      0.99      0.98       789\n",
      "\n",
      "    accuracy                           0.98      1384\n",
      "   macro avg       0.98      0.98      0.98      1384\n",
      "weighted avg       0.98      0.98      0.98      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# 忽略一些导入警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- 1. 数据加载 (公共) ---\n",
    "print(\"正在加载数据 (train.txt)...\")\n",
    "try:\n",
    "    # train.txt\n",
    "    data = pd.read_csv('train.txt', sep='\\t', header=None, names=['label', 'text'], on_bad_lines='skip')\n",
    "except FileNotFoundError:\n",
    "    print(\"错误：train.txt 文件未找到。请确保文件与脚本在同一目录中。\")\n",
    "    exit()\n",
    "\n",
    "texts = data['text'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# --- 2. 数据预处理和划分 (公共) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")\n",
    "print(f\"数据加载完毕。训练集: {len(X_train)} 条, 测试集: {len(X_test)} 条。\")\n",
    "\n",
    "\n",
    "# --- 3. 构建词汇表 (公共) ---\n",
    "tokenizer = lambda x: str(x).split()\n",
    "all_tokens = [token for text in X_train for token in tokenizer(text)]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "vocab_limit = 5000\n",
    "vocab_list = token_counts.most_common(vocab_limit)\n",
    "word_to_idx = {word: i + 2 for i, (word, _) in enumerate(vocab_list)}\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = 1\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "PAD_IDX = word_to_idx['<PAD>']\n",
    "\n",
    "# --- 4. 文本编码与数据集定义 (公共) ---\n",
    "def text_encoder(text):\n",
    "    tokens = tokenizer(str(text))\n",
    "    indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [text_encoder(text) for text in texts]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=PAD_IDX)\n",
    "    labels = torch.stack(labels, 0)\n",
    "    return texts_padded, labels\n",
    "\n",
    "# --- 5. 定义模型 1 (LSTM + 平均池化) ---\n",
    "class LSTMAveragePoolingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text 形状: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded 形状: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # lstm_out 形状: [batch_size, seq_len, hidden_dim]\n",
    "        lstm_out, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # 对所有时间步的输出向量进行平均池化\n",
    "        pooled = torch.mean(lstm_out, dim=1)\n",
    "        # pooled 形状: [batch_size, hidden_dim]\n",
    "        \n",
    "        output = self.fc(pooled)\n",
    "        return output\n",
    "\n",
    "# --- 6. 定义模型 2 (CNN-LSTM 拼接) ---\n",
    "# 参照 LSTM-classification.html 和 TextCNN-2classes-torch.html 的架构\n",
    "class CNN_LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_filters, filter_sizes, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # CNN 分支 (参照 TextCNN-2classes-torch.html)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=n_filters, \n",
    "                      kernel_size=(fs, embedding_dim)) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # LSTM 分支 (参照 LSTM-classification.html，但按要求修改为平均池化)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 整合层\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters + hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # --- LSTM 路径 ---\n",
    "        lstm_out, (hn, cn) = self.lstm(embedded)\n",
    "        lstm_pooled = torch.mean(lstm_out, dim=1)\n",
    "        \n",
    "        # --- CNN 路径 ---\n",
    "        embedded_cnn = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded_cnn)) for conv in self.convs]\n",
    "        conved = [c.squeeze(3) for c in conved]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]) for conv in conved]\n",
    "        pooled = [p.squeeze(2) for p in pooled]\n",
    "        cnn_features = torch.cat(pooled, dim=1)\n",
    "        \n",
    "        # --- 整合 (拼接) ---\n",
    "        combined = torch.cat((cnn_features, lstm_pooled), dim=1)\n",
    "        combined_dropped = self.dropout(combined)\n",
    "        \n",
    "        output = self.fc(combined_dropped)\n",
    "        return output\n",
    "\n",
    "# --- 7. 公共设置 ---\n",
    "# 超参数\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2  # 二分类\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# CNN 特定超参数\n",
    "CNN_N_FILTERS = 100\n",
    "CNN_FILTER_SIZES = [3, 4, 5]\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# 设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 公共的数据集和DataLoader\n",
    "train_dataset = TextClassificationDataset(X_train, y_train)\n",
    "test_dataset = TextClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# --- 8. 运行实验 1 ---\n",
    "print(\"\\n--- 开始 实验 1 (LSTM + 输出均值池化) ---\")\n",
    "model_exp1 = LSTMAveragePoolingModel(vocab_size, EMBEDDING_DIM, LSTM_HIDDEN_DIM, OUTPUT_DIM, PAD_IDX).to(device)\n",
    "criterion_exp1 = nn.CrossEntropyLoss().to(device)\n",
    "optimizer_exp1 = optim.Adam(model_exp1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_exp1.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer_exp1.zero_grad()\n",
    "        predictions = model_exp1(texts)\n",
    "        loss = criterion_exp1(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer_exp1.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1:02}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"实验 1 训练完成。\")\n",
    "\n",
    "# 评估实验 1\n",
    "model_exp1.eval()\n",
    "all_preds_exp1 = []\n",
    "all_labels_exp1 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model_exp1(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds_exp1.extend(predicted.cpu().numpy())\n",
    "        all_labels_exp1.extend(labels.cpu().numpy())\n",
    "\n",
    "target_names = [f\"Class {label}\" for label in np.unique(labels.cpu().numpy())]\n",
    "report_exp1 = classification_report(all_labels_exp1, all_preds_exp1, target_names=target_names)\n",
    "print(\"\\n--- 实验 1 评测报告 (LSTM + 所有步长输出平均池化) ---\")\n",
    "print(report_exp1)\n",
    "\n",
    "\n",
    "# --- 9. 运行实验 2 ---\n",
    "print(\"\\n--- 开始 实验 2 (CNN + LSTM 拼接) ---\")\n",
    "model_exp2 = CNN_LSTM_Model(\n",
    "    vocab_size, EMBEDDING_DIM, LSTM_HIDDEN_DIM, OUTPUT_DIM, \n",
    "    CNN_N_FILTERS, CNN_FILTER_SIZES, DROPOUT, PAD_IDX\n",
    ").to(device)\n",
    "criterion_exp2 = nn.CrossEntropyLoss().to(device)\n",
    "optimizer_exp2 = optim.Adam(model_exp2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_exp2.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer_exp2.zero_grad()\n",
    "        predictions = model_exp2(texts)\n",
    "        loss = criterion_exp2(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer_exp2.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1:02}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"实验 2 训练完成。\")\n",
    "\n",
    "# 评估实验 2\n",
    "model_exp2.eval()\n",
    "all_preds_exp2 = []\n",
    "all_labels_exp2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model_exp2(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds_exp2.extend(predicted.cpu().numpy())\n",
    "        all_labels_exp2.extend(labels.cpu().numpy())\n",
    "\n",
    "report_exp2 = classification_report(all_labels_exp2, all_preds_exp2, target_names=target_names)\n",
    "print(\"\\n--- 实验 2 评测报告 (CNN + LSTM 拼接) ---\")\n",
    "print(report_exp2)\n",
    "\n",
    "# --- 10. 最终性能比较 ---\n",
    "print(\"\\n--- 实验 1 与 实验 2 性能比较 ---\")\n",
    "print(\"\\n实验 1 结果 (LSTM + 输出均值池化):**\")\n",
    "print(report_exp1)\n",
    "print(\"\\n实验 2 结果 (CNN + LSTM 拼接):**\")\n",
    "print(report_exp2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f07ced",
   "metadata": {},
   "source": [
    "性能比较分析：\n",
    "1. 实验1 (LSTM + 输出均值池化) 表现非常出色，在测试集上达到了约 98% 的准确率。\n",
    "2. 实验2 (CNN + LSTM 拼接) 在实验1的基础上，性能有轻微提升，准确率达到了约 99%。\n",
    "3. 结论：\n",
    "LSTM 擅长捕捉序列中的长期依赖和上下文关系。\n",
    "CNN 擅长提取局部的n-gram特征（例如关键短语）。\n",
    "通过将两种模型的特征向量拼接（实验2），模型能够同时利用LSTM的上下文理解能力和CNN的局部关键特征提取能力。这种互补性使得整合模型的性能略微优于单独的LSTM模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
