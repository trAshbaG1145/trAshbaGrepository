{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9ab1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267458dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len  42\n",
      "nb_words  2270\n"
     ]
    }
   ],
   "source": [
    "# 统计 train.txt 中最长句子的长度 maxlen; 句子条数 num_recs; 每词的词频 word_freqs[]\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "with open('./data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        words = nltk.word_tokenize(sentence.lower()) #逐句分词\n",
    "        maxlen = max(maxlen, len(words))    #最长句子\n",
    "        word_freqs.update(words)\n",
    "        num_recs += 1\n",
    "\n",
    "print('max_len ', maxlen)   #最长句子\n",
    "print('nb_words ', len(word_freqs))#单词数，为了统计词典大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6eec2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成单词表和数字表\n",
    "MAX_FEATURES = 2000   #词典大小\n",
    "MAX_SENTENCE_LENGTH = 40   #句子长度限定为40\n",
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2  #加入填充词，和未知词\n",
    "\n",
    "word2index = {x[0]: i + 2 for i, x in enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "\n",
    "# 数据准备\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                label, sentence = line.strip().split(\"\\t\")\n",
    "                words = nltk.word_tokenize(sentence.lower())\n",
    "                seqs = [word2index.get(word, word2index[\"UNK\"]) for word in words]\n",
    "                self.X.append(seqs)\n",
    "                self.y.append(int(label))\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]), self.y[idx]\n",
    "\n",
    "# 实例化数据集\n",
    "dataset = TextDataset('./data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f987e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列\n",
    "def pad_sequences(sequences, maxlen):  #填充句子\n",
    "    return [torch.tensor(seq + [word2index[\"PAD\"]] * (maxlen - len(seq)) if len(seq) < maxlen else seq[:maxlen]) for seq in sequences]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(dataset.X, dataset.y.numpy(), test_size=0.2, random_state=42)\n",
    "Xtrain = pad_sequences(Xtrain, MAX_SENTENCE_LENGTH) #按40最大长度来填充\n",
    "Xtest = pad_sequences(Xtest, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "# 创建 DataLoader，按batchsize分批\n",
    "train_loader = DataLoader(list(zip(Xtrain, ytrain)), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(Xtest, ytest)), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1105ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6867\n",
      "Epoch 2, Loss: 0.5002\n",
      "Epoch 3, Loss: 0.2772\n",
      "Epoch 4, Loss: 0.2414\n",
      "Epoch 5, Loss: 0.2173\n",
      "Epoch 6, Loss: 0.2006\n",
      "Epoch 7, Loss: 0.1891\n",
      "Epoch 8, Loss: 0.1617\n",
      "Epoch 9, Loss: 0.1544\n",
      "Epoch 10, Loss: 0.2410\n",
      "Epoch 11, Loss: 0.1508\n",
      "Epoch 12, Loss: 0.1552\n",
      "Epoch 13, Loss: 0.1218\n",
      "Epoch 14, Loss: 0.1157\n",
      "Epoch 15, Loss: 0.1078\n",
      "Epoch 16, Loss: 0.1123\n",
      "Epoch 17, Loss: 0.1264\n",
      "Epoch 18, Loss: 0.1034\n",
      "Epoch 19, Loss: 0.0991\n",
      "Epoch 20, Loss: 0.0905\n"
     ]
    }
   ],
   "source": [
    "# 定义 RNN 模型\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_output, hidden = self.rnn(embedded)#每个时间步的预测向量和隐向量\n",
    "        output = self.fc(hidden[-1])\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "# 初始化模型\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "model = RNN(vocab_size, EMBEDDING_SIZE, HIDDEN_LAYER_SIZE)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "NUM_EPOCHS = 20\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db8bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9147\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3154ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "积极   I love reading.\n",
      "消极   You are so boring.\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "INPUT_SENTENCES = ['I love reading.', 'You are so boring.']\n",
    "XX = []\n",
    "\n",
    "for sentence in INPUT_SENTENCES:\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seq = [word2index.get(word, word2index['UNK']) for word in words]\n",
    "    XX.append(seq)\n",
    "\n",
    "XX = pad_sequences(XX, MAX_SENTENCE_LENGTH)\n",
    "labels = model(torch.stack(XX)).detach().numpy()\n",
    "labels = [int(round(x[0])) for x in labels]\n",
    "\n",
    "label2word = {1: '积极', 0: '消极'}\n",
    "for i in range(len(INPUT_SENTENCES)):\n",
    "    print('{}   {}'.format(label2word[labels[i]], INPUT_SENTENCES[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9d562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
