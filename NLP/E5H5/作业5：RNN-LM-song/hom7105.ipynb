{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "作业\n",
    "参考案例代码，完成指定任务。案例RNN-LM-song中的程序是一个根据歌词语料data.txt生成的RNN语言模型，它可以生成指定长度的相同风格的歌词。请参考该案例，收集某一类风格的歌词，或者诗歌，编写个RNN语言模型，并进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "语料库总字数: 25869\n",
      "词汇表大小: 2491\n",
      "\n",
      "开始训练...\n",
      "  批次 100/808, Loss: 6.3675\n",
      "  批次 200/808, Loss: 6.1105\n",
      "  批次 300/808, Loss: 5.6957\n",
      "  批次 400/808, Loss: 5.2936\n",
      "  批次 500/808, Loss: 4.8081\n",
      "  批次 600/808, Loss: 4.7293\n",
      "  批次 700/808, Loss: 4.3257\n",
      "  批次 800/808, Loss: 3.9525\n",
      "--- Epoch 1/30 完成, 平均 Loss: 5.3732 ---\n",
      "  批次 100/808, Loss: 3.5367\n",
      "  批次 200/808, Loss: 3.4780\n",
      "  批次 300/808, Loss: 3.1143\n",
      "  批次 400/808, Loss: 2.8966\n",
      "  批次 500/808, Loss: 2.5326\n",
      "  批次 600/808, Loss: 2.4987\n",
      "  批次 700/808, Loss: 2.5066\n",
      "  批次 800/808, Loss: 2.0920\n",
      "--- Epoch 2/30 完成, 平均 Loss: 2.9456 ---\n",
      "  批次 100/808, Loss: 1.9191\n",
      "  批次 200/808, Loss: 1.8137\n",
      "  批次 300/808, Loss: 1.6639\n",
      "  批次 400/808, Loss: 1.5365\n",
      "  批次 500/808, Loss: 1.4701\n",
      "  批次 600/808, Loss: 1.3435\n",
      "  批次 700/808, Loss: 1.3320\n",
      "  批次 800/808, Loss: 1.2313\n",
      "--- Epoch 3/30 完成, 平均 Loss: 1.5895 ---\n",
      "  批次 100/808, Loss: 1.1239\n",
      "  批次 200/808, Loss: 0.9966\n",
      "  批次 300/808, Loss: 0.9619\n",
      "  批次 400/808, Loss: 0.9052\n",
      "  批次 500/808, Loss: 0.8093\n",
      "  批次 600/808, Loss: 0.9511\n",
      "  批次 700/808, Loss: 0.8382\n",
      "  批次 800/808, Loss: 0.7276\n",
      "--- Epoch 4/30 完成, 平均 Loss: 0.9250 ---\n",
      "  批次 100/808, Loss: 0.6926\n",
      "  批次 200/808, Loss: 0.5698\n",
      "  批次 300/808, Loss: 0.6436\n",
      "  批次 400/808, Loss: 0.6030\n",
      "  批次 500/808, Loss: 0.6787\n",
      "  批次 600/808, Loss: 0.5309\n",
      "  批次 700/808, Loss: 0.5674\n",
      "  批次 800/808, Loss: 0.5520\n",
      "--- Epoch 5/30 完成, 平均 Loss: 0.6222 ---\n",
      "\n",
      "--- Epoch 5 测试生成 ---\n",
      "  以 '春眠' 开头: 春眠。迟钟千树万树梨花开。散入珠帘湿罗幕，狐裘不暖锦衾薄。将军角弓不得控，都护铁衣冷\n",
      "  以 '明月' 开头: 明月，低头思故乡。美人卷珠帘，深坐蹙蛾眉。但见泪痕湿，不知心恨谁。向晚意不适，驱车登\n",
      "--------------------------\n",
      "\n",
      "  批次 100/808, Loss: 0.4669\n",
      "  批次 200/808, Loss: 0.4382\n",
      "  批次 300/808, Loss: 0.5182\n",
      "  批次 400/808, Loss: 0.5397\n",
      "  批次 500/808, Loss: 0.5026\n",
      "  批次 600/808, Loss: 0.5144\n",
      "  批次 700/808, Loss: 0.4545\n",
      "  批次 800/808, Loss: 0.4759\n",
      "--- Epoch 6/30 完成, 平均 Loss: 0.4871 ---\n",
      "  批次 100/808, Loss: 0.4728\n",
      "  批次 200/808, Loss: 0.4457\n",
      "  批次 300/808, Loss: 0.4576\n",
      "  批次 400/808, Loss: 0.4416\n",
      "  批次 500/808, Loss: 0.3903\n",
      "  批次 600/808, Loss: 0.4199\n",
      "  批次 700/808, Loss: 0.3741\n",
      "  批次 800/808, Loss: 0.4366\n",
      "--- Epoch 7/30 完成, 平均 Loss: 0.4188 ---\n",
      "  批次 100/808, Loss: 0.3424\n",
      "  批次 200/808, Loss: 0.3801\n",
      "  批次 300/808, Loss: 0.3977\n",
      "  批次 400/808, Loss: 0.3706\n",
      "  批次 500/808, Loss: 0.4111\n",
      "  批次 600/808, Loss: 0.3541\n",
      "  批次 700/808, Loss: 0.3683\n",
      "  批次 800/808, Loss: 0.3409\n",
      "--- Epoch 8/30 完成, 平均 Loss: 0.3777 ---\n",
      "  批次 100/808, Loss: 0.3331\n",
      "  批次 200/808, Loss: 0.3066\n",
      "  批次 300/808, Loss: 0.3140\n",
      "  批次 400/808, Loss: 0.3524\n",
      "  批次 500/808, Loss: 0.3246\n",
      "  批次 600/808, Loss: 0.3645\n",
      "  批次 700/808, Loss: 0.3702\n",
      "  批次 800/808, Loss: 0.3271\n",
      "--- Epoch 9/30 完成, 平均 Loss: 0.3508 ---\n",
      "  批次 100/808, Loss: 0.2989\n",
      "  批次 200/808, Loss: 0.3288\n",
      "  批次 300/808, Loss: 0.3347\n",
      "  批次 400/808, Loss: 0.3264\n",
      "  批次 500/808, Loss: 0.2968\n",
      "  批次 600/808, Loss: 0.3391\n",
      "  批次 700/808, Loss: 0.3213\n",
      "  批次 800/808, Loss: 0.3201\n",
      "--- Epoch 10/30 完成, 平均 Loss: 0.3326 ---\n",
      "\n",
      "--- Epoch 10 测试生成 ---\n",
      "  以 '春眠' 开头: 春眠。迟迟钟鼓初长夜，耿耿星河欲曙天。鸳鸯瓦冷霜华重，翡翠衾寒谁与共？悠悠生死别经年\n",
      "  以 '明月' 开头: 明月，低头思故乡。美人卷珠帘，深坐蹙蛾眉。但见泪痕湿，不知心恨谁。向晚意不适，驱车登\n",
      "--------------------------\n",
      "\n",
      "  批次 100/808, Loss: 0.2959\n",
      "  批次 200/808, Loss: 0.3180\n",
      "  批次 300/808, Loss: 0.3350\n",
      "  批次 400/808, Loss: 0.2879\n",
      "  批次 500/808, Loss: 0.3457\n",
      "  批次 600/808, Loss: 0.3552\n",
      "  批次 700/808, Loss: 0.3177\n",
      "  批次 800/808, Loss: 0.2979\n",
      "--- Epoch 11/30 完成, 平均 Loss: 0.3195 ---\n",
      "  批次 100/808, Loss: 0.2621\n",
      "  批次 200/808, Loss: 0.3236\n",
      "  批次 300/808, Loss: 0.2720\n",
      "  批次 400/808, Loss: 0.3252\n",
      "  批次 500/808, Loss: 0.2831\n",
      "  批次 600/808, Loss: 0.2702\n",
      "  批次 700/808, Loss: 0.2907\n",
      "  批次 800/808, Loss: 0.3254\n",
      "--- Epoch 12/30 完成, 平均 Loss: 0.3098 ---\n",
      "  批次 100/808, Loss: 0.2410\n",
      "  批次 200/808, Loss: 0.2778\n",
      "  批次 300/808, Loss: 0.2893\n",
      "  批次 400/808, Loss: 0.3389\n",
      "  批次 500/808, Loss: 0.3553\n",
      "  批次 600/808, Loss: 0.2944\n",
      "  批次 700/808, Loss: 0.2828\n",
      "  批次 800/808, Loss: 0.3143\n",
      "--- Epoch 13/30 完成, 平均 Loss: 0.3025 ---\n",
      "  批次 100/808, Loss: 0.2595\n",
      "  批次 200/808, Loss: 0.2682\n",
      "  批次 300/808, Loss: 0.2433\n",
      "  批次 400/808, Loss: 0.2937\n",
      "  批次 500/808, Loss: 0.2917\n",
      "  批次 600/808, Loss: 0.2619\n",
      "  批次 700/808, Loss: 0.3060\n",
      "  批次 800/808, Loss: 0.3481\n",
      "--- Epoch 14/30 完成, 平均 Loss: 0.2966 ---\n",
      "  批次 100/808, Loss: 0.2517\n",
      "  批次 200/808, Loss: 0.2479\n",
      "  批次 300/808, Loss: 0.3089\n",
      "  批次 400/808, Loss: 0.3142\n",
      "  批次 500/808, Loss: 0.3169\n",
      "  批次 600/808, Loss: 0.3180\n",
      "  批次 700/808, Loss: 0.3456\n",
      "  批次 800/808, Loss: 0.2624\n",
      "--- Epoch 15/30 完成, 平均 Loss: 0.2928 ---\n",
      "\n",
      "--- Epoch 15 测试生成 ---\n",
      "  以 '春眠' 开头: 春眠客心。心断新丰酒，销愁斗几千。高阁客竟去，小园花乱飞。参差连曲陌，迢递送斜晖。肠\n",
      "  以 '明月' 开头: 明月，家书抵万金。白头搔更短，浑欲不胜簪。花隐掖垣暮，啾啾栖鸟过。星临万户动，月傍九\n",
      "--------------------------\n",
      "\n",
      "  批次 100/808, Loss: 0.2562\n",
      "  批次 200/808, Loss: 0.3019\n",
      "  批次 300/808, Loss: 0.2790\n",
      "  批次 400/808, Loss: 0.2465\n",
      "  批次 500/808, Loss: 0.3073\n",
      "  批次 600/808, Loss: 0.3189\n",
      "  批次 700/808, Loss: 0.3028\n",
      "  批次 800/808, Loss: 0.2948\n",
      "--- Epoch 16/30 完成, 平均 Loss: 0.2896 ---\n",
      "  批次 100/808, Loss: 0.2726\n",
      "  批次 200/808, Loss: 0.3030\n",
      "  批次 300/808, Loss: 0.3316\n",
      "  批次 400/808, Loss: 0.2962\n",
      "  批次 500/808, Loss: 0.2989\n",
      "  批次 600/808, Loss: 0.2748\n",
      "  批次 700/808, Loss: 0.3022\n",
      "  批次 800/808, Loss: 0.2816\n",
      "--- Epoch 17/30 完成, 平均 Loss: 0.2865 ---\n",
      "  批次 100/808, Loss: 0.2686\n",
      "  批次 200/808, Loss: 0.2583\n",
      "  批次 300/808, Loss: 0.2670\n",
      "  批次 400/808, Loss: 0.2569\n",
      "  批次 500/808, Loss: 0.3111\n",
      "  批次 600/808, Loss: 0.3177\n",
      "  批次 700/808, Loss: 0.2626\n",
      "  批次 800/808, Loss: 0.2945\n",
      "--- Epoch 18/30 完成, 平均 Loss: 0.2841 ---\n",
      "  批次 100/808, Loss: 0.2625\n",
      "  批次 200/808, Loss: 0.2392\n",
      "  批次 300/808, Loss: 0.2984\n",
      "  批次 400/808, Loss: 0.3162\n",
      "  批次 500/808, Loss: 0.2813\n",
      "  批次 600/808, Loss: 0.2760\n",
      "  批次 700/808, Loss: 0.2892\n",
      "  批次 800/808, Loss: 0.3216\n",
      "--- Epoch 19/30 完成, 平均 Loss: 0.2821 ---\n",
      "  批次 100/808, Loss: 0.2878\n",
      "  批次 200/808, Loss: 0.2731\n",
      "  批次 300/808, Loss: 0.2876\n",
      "  批次 400/808, Loss: 0.2984\n",
      "  批次 500/808, Loss: 0.2743\n",
      "  批次 600/808, Loss: 0.2762\n",
      "  批次 700/808, Loss: 0.2567\n",
      "  批次 800/808, Loss: 0.2850\n",
      "--- Epoch 20/30 完成, 平均 Loss: 0.2806 ---\n",
      "\n",
      "--- Epoch 20 测试生成 ---\n",
      "  以 '春眠' 开头: 春眠客心。心逐青溪几度到云林。春来遍是桃花水，不辨仙源何处寻。云想衣裳花想容，春风拂\n",
      "  以 '明月' 开头: 明月，家书抵万金。白头搔更短，浑欲不胜簪。花隐掖垣暮，啾啾栖鸟过。星临万户动，月傍九\n",
      "--------------------------\n",
      "\n",
      "  批次 100/808, Loss: 0.2740\n",
      "  批次 200/808, Loss: 0.2680\n",
      "  批次 300/808, Loss: 0.2978\n",
      "  批次 400/808, Loss: 0.2950\n",
      "  批次 500/808, Loss: 0.2990\n",
      "  批次 600/808, Loss: 0.2804\n",
      "  批次 700/808, Loss: 0.2970\n",
      "  批次 800/808, Loss: 0.2647\n",
      "--- Epoch 21/30 完成, 平均 Loss: 0.2792 ---\n",
      "  批次 100/808, Loss: 0.2562\n",
      "  批次 200/808, Loss: 0.2888\n",
      "  批次 300/808, Loss: 0.2803\n",
      "  批次 400/808, Loss: 0.3287\n",
      "  批次 500/808, Loss: 0.3093\n",
      "  批次 600/808, Loss: 0.3179\n",
      "  批次 700/808, Loss: 0.3126\n",
      "  批次 800/808, Loss: 0.2453\n",
      "--- Epoch 22/30 完成, 平均 Loss: 0.2779 ---\n",
      "  批次 100/808, Loss: 0.2638\n",
      "  批次 200/808, Loss: 0.2488\n",
      "  批次 300/808, Loss: 0.2800\n",
      "  批次 400/808, Loss: 0.2977\n",
      "  批次 500/808, Loss: 0.2997\n",
      "  批次 600/808, Loss: 0.2768\n",
      "  批次 700/808, Loss: 0.2637\n",
      "  批次 800/808, Loss: 0.3064\n",
      "--- Epoch 23/30 完成, 平均 Loss: 0.2771 ---\n",
      "  批次 100/808, Loss: 0.2450\n",
      "  批次 200/808, Loss: 0.2396\n",
      "  批次 300/808, Loss: 0.2334\n",
      "  批次 400/808, Loss: 0.2991\n",
      "  批次 500/808, Loss: 0.2930\n",
      "  批次 600/808, Loss: 0.2829\n",
      "  批次 700/808, Loss: 0.2782\n",
      "  批次 800/808, Loss: 0.2604\n",
      "--- Epoch 24/30 完成, 平均 Loss: 0.2759 ---\n",
      "  批次 100/808, Loss: 0.2163\n",
      "  批次 200/808, Loss: 0.2436\n",
      "  批次 300/808, Loss: 0.3072\n",
      "  批次 400/808, Loss: 0.2764\n",
      "  批次 500/808, Loss: 0.2792\n",
      "  批次 600/808, Loss: 0.3325\n",
      "  批次 700/808, Loss: 0.2881\n",
      "  批次 800/808, Loss: 0.2270\n",
      "--- Epoch 25/30 完成, 平均 Loss: 0.2749 ---\n",
      "\n",
      "--- Epoch 25 测试生成 ---\n",
      "  以 '春眠' 开头: 春眠客心，万方尽思乡县。出洞无论隔山水，辞家终拟长游衍。自谓经过旧不迷，安知峰壑今来\n",
      "  以 '明月' 开头: 明月，闺中只独看。遥怜小儿女，未解忆长安。香雾云鬟湿，清辉玉臂寒。何时倚虚幌，双照泪\n",
      "--------------------------\n",
      "\n",
      "  批次 100/808, Loss: 0.2537\n",
      "  批次 200/808, Loss: 0.2593\n",
      "  批次 300/808, Loss: 0.2794\n",
      "  批次 400/808, Loss: 0.2873\n",
      "  批次 500/808, Loss: 0.2687\n",
      "  批次 600/808, Loss: 0.2318\n",
      "  批次 700/808, Loss: 0.2866\n",
      "  批次 800/808, Loss: 0.2834\n",
      "--- Epoch 26/30 完成, 平均 Loss: 0.2741 ---\n",
      "  批次 100/808, Loss: 0.2729\n",
      "  批次 200/808, Loss: 0.2795\n",
      "  批次 300/808, Loss: 0.2816\n",
      "  批次 400/808, Loss: 0.2757\n",
      "  批次 500/808, Loss: 0.3003\n",
      "  批次 600/808, Loss: 0.2664\n",
      "  批次 700/808, Loss: 0.2878\n",
      "  批次 800/808, Loss: 0.2828\n",
      "--- Epoch 27/30 完成, 平均 Loss: 0.2733 ---\n",
      "  批次 100/808, Loss: 0.2691\n",
      "  批次 200/808, Loss: 0.2375\n",
      "  批次 300/808, Loss: 0.3045\n",
      "  批次 400/808, Loss: 0.2427\n",
      "  批次 500/808, Loss: 0.2279\n",
      "  批次 600/808, Loss: 0.3025\n",
      "  批次 700/808, Loss: 0.2995\n",
      "  批次 800/808, Loss: 0.2910\n",
      "--- Epoch 28/30 完成, 平均 Loss: 0.2726 ---\n",
      "  批次 100/808, Loss: 0.2341\n",
      "  批次 200/808, Loss: 0.2440\n",
      "  批次 300/808, Loss: 0.2702\n",
      "  批次 400/808, Loss: 0.2838\n",
      "  批次 500/808, Loss: 0.2648\n",
      "  批次 600/808, Loss: 0.2469\n",
      "  批次 700/808, Loss: 0.2534\n",
      "  批次 800/808, Loss: 0.3236\n",
      "--- Epoch 29/30 完成, 平均 Loss: 0.2717 ---\n",
      "  批次 100/808, Loss: 0.2028\n",
      "  批次 200/808, Loss: 0.2690\n",
      "  批次 300/808, Loss: 0.2607\n",
      "  批次 400/808, Loss: 0.2386\n",
      "  批次 500/808, Loss: 0.2718\n",
      "  批次 600/808, Loss: 0.2588\n",
      "  批次 700/808, Loss: 0.2833\n",
      "  批次 800/808, Loss: 0.2898\n",
      "--- Epoch 30/30 完成, 平均 Loss: 0.2713 ---\n",
      "\n",
      "--- Epoch 30 测试生成 ---\n",
      "  以 '春眠' 开头: 春眠客心。心断无消息石榴红。斑骓只系垂杨岸，何处西南任好风。相见时难别亦难，东风无力\n",
      "  以 '明月' 开头: 明月，低头思故乡。美人卷珠帘，深坐蹙蛾眉。但见泪痕湿，不知心恨谁。向晚意不适，驱车登\n",
      "--------------------------\n",
      "\n",
      "训练完成。\n",
      "\n",
      "--- 最终生成测试 ---\n",
      "白日，七层摩苍穹。下窥指高鸟，俯听闻惊风。连山若波涛，奔凑似朝东。秋色从西来，苍然满关中。五陵北原上，万古青濛濛。净理了可悟\n",
      "红豆生南国，春来发几枝。愿君多采撷，此物最相思。近寒食雨草萋萋，著麦苗风柳映堤。等是有家归未得，杜鹃休向耳边啼。空山不见人，\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# ---  参数设置 (参照 RNN-LM-song.html) ---\n",
    "SEQ_LEN = 16        # 序列长度\n",
    "BATCH_SIZE = 32     # 批处理大小\n",
    "EMBEDDING_DIM = 128 # 嵌入层维度\n",
    "HIDDEN_DIM = 256    # LSTM 隐藏层维度\n",
    "NUM_EPOCHS = 30     # 训练周期\n",
    "LR = 0.001          # 学习率\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"使用设备: {DEVICE}\")\n",
    "\n",
    "# ---  数据处理 (参照 RNN-LM-song.html) ---\n",
    "def load_data(file_path):\n",
    "    \"\"\"加载并预处理数据\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # 参照案例，替换换行符和空格\n",
    "    data = re.sub(r'[\\n\\r\\s]', '', data)\n",
    "    print(f\"语料库总字数: {len(data)}\")\n",
    "    \n",
    "    # 构建词汇表 (字符级)\n",
    "    words = sorted(list(set(data)))\n",
    "    word_to_ix = {word: ix for ix, word in enumerate(words)}\n",
    "    ix_to_word = {ix: word for ix, word in enumerate(words)}\n",
    "    vocab_size = len(word_to_ix)\n",
    "    \n",
    "    print(f\"词汇表大小: {vocab_size}\")\n",
    "    return data, word_to_ix, ix_to_word, vocab_size\n",
    "\n",
    "# ---  Dataset 和 DataLoader (参照 RNN-LM-song.html) ---\n",
    "class PoetryDataset(Dataset):\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "    def __init__(self, data, word_to_ix, seq_len):\n",
    "        self.data = data\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = len(word_to_ix)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 定义输入序列 (x) 和目标序列 (y)\n",
    "        # y 是 x 向后错位一个字符\n",
    "        data_seq = self.data[i: i + self.seq_len]\n",
    "        label_seq = self.data[i + 1: i + 1 + self.seq_len]\n",
    "        \n",
    "        # 转换为索引\n",
    "        data_ix = [self.word_to_ix[char] for char in data_seq]\n",
    "        label_ix = [self.word_to_ix[char] for char in label_seq]\n",
    "        \n",
    "        return torch.tensor(data_ix), torch.tensor(label_ix)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 使用滑动窗口，总长度为 data_len - seq_len\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "# ---  RNN (LSTM) 模型 (参照 RNN-LM-song.html) ---\n",
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 案例中使用 LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(embeds)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # lstm_out: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 将每个时间步的输出都通过全连接层\n",
    "        output = self.fc(lstm_out)\n",
    "        # output: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # 初始化 LSTM 的 hidden state 和 cell state\n",
    "        return (torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(1, batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "# ---  训练函数 (参照 RNN-LM-song.html) ---\n",
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (data, label) in enumerate(data_loader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        hidden = model.init_hidden(data.size(0), device)\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        # LSTM的输出是 [batch_size, seq_len, vocab_size]\n",
    "        # 损失函数需要 [batch_size * seq_len, vocab_size]\n",
    "        # label 需要 [batch_size * seq_len]\n",
    "        loss = criterion(output.view(-1, model.embedding.num_embeddings), label.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(f\"  批次 {i}/{len(data_loader)}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# ---  生成函数 (参照 RNN-LM-song.html) ---\n",
    "def generate(model, start_word, length, ix_to_word, word_to_ix, device):\n",
    "    \"\"\"\n",
    "    生成文本\n",
    "    参照案例中的 'generate' 函数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = list(start_word)\n",
    "    \n",
    "    # 准备初始输入\n",
    "    input_tensor = torch.tensor([word_to_ix[start_word[0]]]).view(1, 1).to(device)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    # \"预热\" RNN，使用 'start_word' 来初始化隐藏状态\n",
    "    for i in range(len(start_word)):\n",
    "        input_tensor = torch.tensor([word_to_ix[start_word[i]]]).view(1, 1).to(device)\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "    \n",
    "    # 获取 'start_word' 最后一个字符的索引\n",
    "    word_ix = word_to_ix[start_word[-1]]\n",
    "    \n",
    "    # 开始生成\n",
    "    for _ in range(length):\n",
    "        input_tensor = torch.tensor([word_ix]).view(1, 1).to(device)\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        \n",
    "        # 参照案例，使用 topk(1) (即 argmax) 来选择下一个词\n",
    "        # output 形状: [1, 1, vocab_size]\n",
    "        output_data = output.data[0].view(-1)\n",
    "        \n",
    "        # 采用 argmax (最可能的下一个词)\n",
    "        top_index = output_data.topk(1)[1].item()\n",
    "        \n",
    "        word_ix = top_index\n",
    "        word = ix_to_word[word_ix]\n",
    "        results.append(word)\n",
    "\n",
    "    return \"\".join(results)\n",
    "\n",
    "# ---  主执行逻辑 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 加载数据和词汇表\n",
    "    data, word_to_ix, ix_to_word, vocab_size = load_data('poetry_corpus.txt')\n",
    "    \n",
    "    # 创建 Dataset 和 DataLoader\n",
    "    dataset = PoetryDataset(data, word_to_ix, SEQ_LEN)\n",
    "    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = PoetryModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"\\n开始训练...\")\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        avg_loss = train(model, data_loader, optimizer, criterion, DEVICE)\n",
    "        print(f'--- Epoch {epoch}/{NUM_EPOCHS} 完成, 平均 Loss: {avg_loss:.4f} ---')\n",
    "        \n",
    "        # 每5个周期测试一次生成效果\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\n--- Epoch {epoch} 测试生成 ---\")\n",
    "            generated_text = generate(model, '春眠', 40, ix_to_word, word_to_ix, DEVICE)\n",
    "            print(f\"  以 '春眠' 开头: {generated_text}\")\n",
    "            generated_text_2 = generate(model, '明月', 40, ix_to_word, word_to_ix, DEVICE)\n",
    "            print(f\"  以 '明月' 开头: {generated_text_2}\")\n",
    "            print(\"--------------------------\\n\")\n",
    "            \n",
    "    print(\"训练完成。\")\n",
    "    \n",
    "    # --- 9. 最终测试 ---\n",
    "    print(\"\\n--- 最终生成测试 ---\")\n",
    "    print(generate(model, '白日', 60, ix_to_word, word_to_ix, DEVICE))\n",
    "    print(generate(model, '红豆', 60, ix_to_word, word_to_ix, DEVICE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
